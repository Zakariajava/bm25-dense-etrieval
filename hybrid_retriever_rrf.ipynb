{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a390b8a4",
   "metadata": {},
   "source": [
    "# Hybrid Retrieval (BM25 + Dense) with RRF — Concept Overview\n",
    "\n",
    "When a user asks a question, we need to **search through documents** (e.g., articles).\n",
    "\n",
    "There are two popular ways to search:\n",
    "\n",
    "- **BM25 (sparse retrieval)** — based on **exact word matching**.  \n",
    "  It works great when the query and the document share the same words  \n",
    "  *(e.g., “matrícula”, “plazos”, “UV”)*.\n",
    "\n",
    "- **Dense retrieval (embeddings)** — based on **semantic meaning**.  \n",
    "  Even if the words differ, it can match similar meanings  \n",
    "  *(e.g., “inscripción” ≈ “matrícula”)*.\n",
    "\n",
    "Each method fails where the other succeeds.  \n",
    "The usual solution is to **combine both**, increasing recall without sacrificing precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6ad584",
   "metadata": {},
   "source": [
    "# What is a Hybrid Retriever?\n",
    "\n",
    "A **hybrid retriever** runs **two searches in parallel**:\n",
    "\n",
    "1. One using **BM25** — ranking documents by **textual relevance**.\n",
    "2. Another using **embeddings** — ranking by **semantic similarity**.\n",
    "\n",
    "Finally, it **merges both ranked lists** into a single, more reliable ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772bc0f1",
   "metadata": {},
   "source": [
    "# RRF (Reciprocal Rank Fusion)\n",
    "\n",
    "**Reciprocal Rank Fusion (RRF)** is a simple yet powerful rule for combining ranked lists.\n",
    "\n",
    "### Inputs:\n",
    "- **List A** (e.g., BM25) returns documents with ranks: `rank_A(d) = 1, 2, 3, ...` (1 = best)\n",
    "- **List B** (e.g., Dense) returns ranks: `rank_B(d) = 1, 2, 3, ...`\n",
    "\n",
    "### Formula (per document `d`):\n",
    "RRF\\_score(d) = 1/(k + rank_A(d)) + 1/(k + rank_B(d))\n",
    "\n",
    "\n",
    "- **k** is a constant (typically `60`) — it ensures top ranks matter more but prevents one list from dominating.\n",
    "- If a document doesn’t appear in a list, we treat it as having a **very large rank** (or simply no contribution).\n",
    "\n",
    "### Intuition:\n",
    "- A document ranked very high in **either** list gets a **strong boost** (`1/(k+1)`).\n",
    "- A document that ranks moderately in **both** lists also gains a solid combined score.\n",
    "- **RRF is robust** — no need to normalize scores or tune probabilities, just use ranks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d18acc",
   "metadata": {},
   "source": [
    "# Simple Example of RRF\n",
    "\n",
    "Assume we take the **top-3** documents from each retriever:\n",
    "\n",
    "**BM25 (List A)** → `d1`, `d2`, `d3`  \n",
    "**Dense (List B)** → `d2`, `d3`, `d4`  \n",
    "\n",
    "We set `k = 60` (a common choice).\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-step computation\n",
    "\n",
    "| Document | rank_A | rank_B | RRF Score Calculation | Total |\n",
    "|-----------|--------|--------|------------------------|--------|\n",
    "| **d1** | 1 | — | 1/(60+1) + 0 | **1/61** |\n",
    "| **d2** | 2 | 1 | 1/(60+2) + 1/(60+1) | **1/62 + 1/61** |\n",
    "| **d3** | 3 | 2 | 1/(60+3) + 1/(60+2) | **1/63 + 1/62** |\n",
    "| **d4** | — | 3 | 0 + 1/(60+3) | **1/63** |\n",
    "\n",
    "---\n",
    "\n",
    "### Final order (highest to lowest RRF score):\n",
    "1. **d2** → highest combined (top in both lists)  \n",
    "2. **d3** → solid mid-rank in both lists  \n",
    "3. **d1** → strong in BM25 only  \n",
    "4. **d4** → found only in Dense search\n",
    "\n",
    "**d2 wins** because it performs well in *both* search types.  \n",
    "**d1** was best in BM25 but missing in Dense, so it drops behind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2bb106",
   "metadata": {},
   "source": [
    "### 1 — Environment & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84e25f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "#  Environment & Imports\n",
    "# ===============================================\n",
    "\n",
    "# --- Standard library ---\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "# --- Third-party libraries ---\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "from rank_bm25 import BM25Okapi\n",
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    SentenceTransformersTokenTextSplitter,\n",
    ")\n",
    "\n",
    "# --- Environment setup ---\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# configuration expected from .env.\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "CHROMADB_TOKEN = os.environ.get(\"CHROMADB_TOKEN\")\n",
    "CHROMA_TENANT = os.environ.get(\"CHROMA_TENANT\")       \n",
    "CHROMA_DB = os.environ.get(\"CHROMA_DB\")             \n",
    "CHROMA_HOST = os.environ.get(\"CHROMA_HOST\")            \n",
    "CHROMA_COLLECTION = os.environ.get(\"CHROMA_COLLECTION\") \n",
    "\n",
    "# Early fail-fast to surface configuration errors deterministically.\n",
    "assert OPENAI_API_KEY, \"Set OPENAI_API_KEY\"\n",
    "assert CHROMADB_TOKEN and CHROMA_TENANT and CHROMA_DB and CHROMA_COLLECTION, \"Set Chroma Cloud env vars\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa8f1a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI client\n",
    "llm = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Chroma client (Cloud)\n",
    "client = chromadb.CloudClient(\n",
    "    api_key=CHROMADB_TOKEN,\n",
    "    tenant=CHROMA_TENANT,\n",
    "    database=CHROMA_DB,\n",
    ")\n",
    "collection = client.get_or_create_collection(name=CHROMA_COLLECTION)\n",
    "\n",
    "# Minimal tokenizer dedicated to BM25: lowercasing + simple word/symbol segmentation.\n",
    "# this is sufficient for BM25’s bag-of-words scoring.\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    return re.findall(r\"\\w+|\\S\", (text or \"\").lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5638a813",
   "metadata": {},
   "source": [
    "### 2 - Embedding helpers (OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1752830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-3-small\"  \n",
    "CHROMA_GET_LIMIT = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f17b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Vectorize a batch of texts using OpenAI embeddings. \n",
    "    Returns one embedding per input text, preserving order.\"\"\"\n",
    "    if not texts:\n",
    "        return []\n",
    "    # Send the texts to the OpenAI embedding model to generate embeddings.\n",
    "    # 'resp' is an API response object containing metadata + one embedding per text.\n",
    "    resp = llm.embeddings.create(model=EMBEDDING_MODEL, input=texts)\n",
    "    # Extract only the embedding vectors from the response (ignore metadata).\n",
    "    # resp.data is a list of objects; each has an attribute 'embedding' which is a list of floats.\n",
    "    return [d.embedding for d in resp.data]\n",
    "\n",
    "def embed_query(query: str) -> List[float]:\n",
    "    \"\"\"Vectorize a single query (thin wrapper over embed_texts).\"\"\"\n",
    "    return embed_texts([query])[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c148b54",
   "metadata": {},
   "source": [
    "### 3 — PDF Ingestion, Two-Pass Chunking, Embedding, and Upsert to Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1108c3",
   "metadata": {},
   "source": [
    "#### Scan `./data` and extract text from each PDF\n",
    "\n",
    "For each PDF:\n",
    "- read pages, extract text\n",
    "- join per-document text with double newlines\n",
    "- build a minimal metadata record\n",
    "- keep both full document text and per-page text for traceability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b211efaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_doc_id(path: str, mtime: float) -> str:\n",
    "    \"\"\"\n",
    "    Generate a stable, deterministic document ID based on file path and last modification time.\n",
    "    This ensures identical PDFs (same path + unchanged timestamp) get the same ID across runs.\n",
    "    \"\"\"\n",
    "    raw = f\"{os.path.abspath(path)}::{mtime}\".encode(\"utf-8\")\n",
    "    return hashlib.sha1(raw).hexdigest()  # 40-char hex string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c4bddd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25 PDFs under ./data\n",
      "Ingested 25 PDF documents with text.\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "assert os.path.isdir(DATA_DIR), f\"Data folder not found: {DATA_DIR}\"\n",
    "\n",
    "pdf_paths = sorted(glob.glob(os.path.join(DATA_DIR, \"**/*.pdf\"), recursive=True))\n",
    "print(f\"Found {len(pdf_paths)} PDFs under {DATA_DIR}\")\n",
    "\n",
    "raw_docs: List[Dict[str, Any]] = []  # items: {\"doc_id\",\"filename\",\"path\",\"text\",\"pages_text\",\"last_modified\"}\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipping {pdf_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Extract per-page text (strip leading/trailing whitespace)\n",
    "    pages_text: List[str] = []\n",
    "    for p in reader.pages:\n",
    "        t = p.extract_text() or \"\"\n",
    "        t = t.strip()\n",
    "        if t:\n",
    "            pages_text.append(t)\n",
    "\n",
    "    # Join all page texts into a single document text\n",
    "    full_text = \"\\n\\n\".join(pages_text)\n",
    "\n",
    "    # Skip empty documents (e.g., scanned PDFs without OCR)\n",
    "    if not full_text.strip():\n",
    "        print(f\"[WARN] Empty text: {pdf_path}\")\n",
    "        continue\n",
    "\n",
    "    # Metadata\n",
    "    stat = os.stat(pdf_path)\n",
    "    last_modified_ts = stat.st_mtime\n",
    "    last_modified = datetime.fromtimestamp(last_modified_ts).isoformat()\n",
    "    doc_id = stable_doc_id(pdf_path, last_modified_ts)\n",
    "\n",
    "    raw_docs.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"filename\": os.path.basename(pdf_path),\n",
    "        \"path\": os.path.abspath(pdf_path),\n",
    "        \"text\": full_text,\n",
    "        \"pages_text\": pages_text,  # retain per-page traceability\n",
    "        \"last_modified\": last_modified,\n",
    "    })\n",
    "\n",
    "print(f\"Ingested {len(raw_docs)} PDF documents with text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7165aa6",
   "metadata": {},
   "source": [
    "#### Two-pass splitting\n",
    "First pass at character level preserves coarse structure and avoids awkward breaks.\n",
    "\n",
    "Second pass at token level introduces overlap to improve recall near boundaries.\n",
    "\n",
    "character chunks ≈ 1000 (no overlap)\n",
    "\n",
    "token chunks ≈ 384 tokens with 64-token overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe560917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zakaria\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "char_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "token_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    tokens_per_chunk=384,\n",
    "    chunk_overlap=64,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b053c92",
   "metadata": {},
   "source": [
    "#### Split documents into page-aware chunks with rich metadata\n",
    "\n",
    "For each document:\n",
    "\n",
    "iterate page by page\n",
    "\n",
    "apply character-level splitting, then token-level splitting\n",
    "\n",
    "construct chunks with metadata capturing the source PDF and page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a2f18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks_by_page(doc: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Produce page-aware chunks from a document.\n",
    "    Pipeline: page -> character-level segments -> token-level chunks (with overlap).\n",
    "    Each chunk carries metadata sufficient for page-level citation and source tracing.\n",
    "    \"\"\"\n",
    "    chunks: List[Dict[str, Any]] = []\n",
    "    pages: List[str] = doc.get(\"pages_text\") or [doc[\"text\"]]\n",
    "    total_pages = len(pages)\n",
    "\n",
    "    global_chunk_idx = 0  # running index of chunks across the whole document\n",
    "\n",
    "    for page_num, page_text in enumerate(pages, start=1):\n",
    "        if not page_text or not page_text.strip():\n",
    "            continue\n",
    "\n",
    "        # First pass: character-level segmentation\n",
    "        char_segments = char_splitter.split_text(page_text)\n",
    "\n",
    "        # Second pass: token-level segmentation with overlap\n",
    "        page_chunk_idx = 0\n",
    "        for seg in char_segments:\n",
    "            token_chunks = token_splitter.split_text(seg)\n",
    "            for chunk_text in token_chunks:\n",
    "                # Skip degenerate chunks that are purely whitespace\n",
    "                if not chunk_text.strip():\n",
    "                    continue\n",
    "\n",
    "                chunk_id = f\"{doc['doc_id']}::p{page_num}::ch{global_chunk_idx}\"\n",
    "\n",
    "                chunks.append({\n",
    "                    \"id\": chunk_id,\n",
    "                    \"text\": chunk_text,\n",
    "                    \"metadata\": {\n",
    "                        \"doc_id\": doc[\"doc_id\"],\n",
    "                        \"filename\": doc[\"filename\"],\n",
    "                        \"path\": doc[\"path\"],\n",
    "                        \"page\": page_num,                    # 1-based page number for citation\n",
    "                        \"page_chunk_index\": page_chunk_idx,  # chunk index within the page\n",
    "                        \"chunk_index\": global_chunk_idx,     # global chunk index within the doc\n",
    "                        \"total_pages\": total_pages,\n",
    "                        \"last_modified\": doc[\"last_modified\"],\n",
    "                        # Useful anchor for viewers that support page fragments:\n",
    "                        \"source\": f\"{doc['path']}#page={page_num}\",\n",
    "                        # Additional domain-specific fields can be added here (e.g., \"section\", \"language\").\n",
    "                    }\n",
    "                })\n",
    "\n",
    "                page_chunk_idx += 1\n",
    "                global_chunk_idx += 1\n",
    "\n",
    "    print(f\"[{doc['filename']}] produced {len(chunks)} chunks with page metadata.\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9e952b",
   "metadata": {},
   "source": [
    "#### Build the complete chunk list across the corpus\n",
    "\n",
    "Aggregate page-aware chunks from all documents into a single list for embedding and upsert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e6ee450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024.findings-acl.456.pdf] produced 89 chunks with page metadata.\n",
      "[2025.acl-long.366.pdf] produced 111 chunks with page metadata.\n",
      "[2401.10020v3.pdf] produced 80 chunks with page metadata.\n",
      "[2401.10774v3.pdf] produced 107 chunks with page metadata.\n",
      "[2401.15884v3.pdf] produced 69 chunks with page metadata.\n",
      "[2401.18059v1.pdf] produced 92 chunks with page metadata.\n",
      "[2402.01306v4.pdf] produced 95 chunks with page metadata.\n",
      "[2402.13547v2.pdf] produced 88 chunks with page metadata.\n",
      "[2402.13753v1.pdf] produced 78 chunks with page metadata.\n",
      "[2403.03206v1.pdf] produced 111 chunks with page metadata.\n",
      "[2403.19887v2.pdf] produced 57 chunks with page metadata.\n",
      "[2404.16130v2.pdf] produced 105 chunks with page metadata.\n",
      "[2405.04437v3.pdf] produced 109 chunks with page metadata.\n",
      "[2405.14734v3.pdf] produced 131 chunks with page metadata.\n",
      "[2405.21060v1.pdf] produced 216 chunks with page metadata.\n",
      "[2406.04692v1.pdf] produced 61 chunks with page metadata.\n",
      "[2407.08608v2.pdf] produced 81 chunks with page metadata.\n",
      "[2407.21783v3.pdf] produced 423 chunks with page metadata.\n",
      "[2408.00714v2.pdf] produced 198 chunks with page metadata.\n",
      "[2408.03326v3.pdf] produced 184 chunks with page metadata.\n",
      "[2411.02265v3.pdf] produced 77 chunks with page metadata.\n",
      "[2412.03603v6.pdf] produced 117 chunks with page metadata.\n",
      "[2501.15383v1.pdf] produced 85 chunks with page metadata.\n",
      "[2502.11371v1.pdf] produced 69 chunks with page metadata.\n",
      "[2503.20215v1.pdf] produced 89 chunks with page metadata.\n",
      "Total chunks prepared: 2922\n"
     ]
    }
   ],
   "source": [
    "all_chunks: List[Dict[str, Any]] = []\n",
    "for d in raw_docs:\n",
    "    all_chunks.extend(split_into_chunks_by_page(d))\n",
    "\n",
    "print(f\"Total chunks prepared: {len(all_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b79a5e",
   "metadata": {},
   "source": [
    "#### Embed and upsert chunks into Chroma (batched)\n",
    "\n",
    "Procedure:\n",
    "\n",
    "batch the chunk list to respect rate limits\n",
    "\n",
    "compute embeddings for each batch\n",
    "\n",
    "upsert (ids, documents, embeddings, metadatas) into the target Chroma collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0294b0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 2922 chunks into Chroma\n"
     ]
    }
   ],
   "source": [
    "BATCH = 256  # tune based on provider limits and latency characteristics\n",
    "\n",
    "def batched(iterable, n: int):\n",
    "    \"\"\"Yield successive n-sized lists from iterable.\"\"\"\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i : i + n]\n",
    "\n",
    "upserted = 0\n",
    "for batch in batched(all_chunks, BATCH):\n",
    "    texts = [c[\"text\"] for c in batch]\n",
    "    ids   = [c[\"id\"] for c in batch]\n",
    "    metas = [c[\"metadata\"] for c in batch]\n",
    "\n",
    "    # Embeddings (OpenAI-compatible)\n",
    "    embeds = embed_texts(texts)\n",
    "\n",
    "    # Upsert into Chroma\n",
    "    collection.upsert(\n",
    "        ids=ids,\n",
    "        documents=texts,\n",
    "        embeddings=embeds,\n",
    "        metadatas=metas,\n",
    "    )\n",
    "    upserted += len(batch)\n",
    "\n",
    "print(f\"Upserted {upserted} chunks into Chroma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1000f337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: 2501.15383v1.pdf — page 15  (distance=0.5906)\n",
      "Link: c:\\Users\\Zakaria\\Downloads\\bm25-dense-etrieval\\data\\2501.15383v1.pdf#page=15\n",
      "its processing time from 4. 9 minutes to only 68 seconds. these improvements significantly reduce user waiting times for long - sequence tasks. compared to the open - source qwen2. 5 - 1m models, qwen2. 5 - turbo excels in short tasks and achieves competitive results on long - context tasks, while delivering shorter processing times and lower costs. consequently, it offers an excellent balance of ...\n",
      "\n",
      "Source: 2501.15383v1.pdf — page 2  (distance=0.6577)\n",
      "Link: c:\\Users\\Zakaria\\Downloads\\bm25-dense-etrieval\\data\\2501.15383v1.pdf#page=2\n",
      "qwen2. 5 - 14b - instruct - 1m. compared to the 128k versions, these models exhibit significantly enhanced long - context capabilities. additionally, we provide an api - accessible model based on mixture of experts ( moe ), called qwen2. 5 - turbo, which offers performance comparable to gpt - 4o - mini but with longer context, stronger capabilities, and more competitive pricing. beyond the models ...\n",
      "\n",
      "Source: 2501.15383v1.pdf — page 2  (distance=0.7265)\n",
      "Link: c:\\Users\\Zakaria\\Downloads\\bm25-dense-etrieval\\data\\2501.15383v1.pdf#page=2\n",
      "qwen2. 5 - 1m series are developed based on qwen2. 5 models ( yang et al., 2025 ) and support context length up to 1m tokens. it currently includes two dense models for opensource, namely qwen2. 5 - 7b - 1m, qwen2. 5 - 14b - 1m, and a moe model for api service, namely qwen2. 5 - turbo. the qwen2. 5 - 1m models retain the same transformer - based architecture as qwen2. 5, ensuring compati - bility ...\n",
      "\n",
      "Source: 2501.15383v1.pdf — page 3  (distance=0.7355)\n",
      "Link: c:\\Users\\Zakaria\\Downloads\\bm25-dense-etrieval\\data\\2501.15383v1.pdf#page=3\n",
      "table 1 : model architecture and license of qwen2. 5 - 1m open - weight models. models layers heads ( q / kv ) tie embedding context / generation length license 7b 28 28 / 4 no 1m / 8k apache 2. 0 14b 48 40 / 8 no 1m / 8k apache 2. 0 3 pre - training long - context pre - training is computationally intensive and can be expensive. to enhance training efficiency and reduce costs, we focus on optimiz ...\n",
      "\n",
      "Source: 2501.15383v1.pdf — page 4  (distance=0.7864)\n",
      "Link: c:\\Users\\Zakaria\\Downloads\\bm25-dense-etrieval\\data\\2501.15383v1.pdf#page=4\n",
      "sequences to fully realize their potential on relatively shorter tasks. 4 post - training the aim of post - training is to effectively enhance the model ’ s performance on long - context tasks while ensuring that performance on short tasks does not decline. we highlight the following efforts in building the qwen2. 5 - 1m models during post - training : synthesizing long instruction data. in long - ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_text = \"What did Qwen2.5-1M models do to enhance training efficiency and reduce costs?\"\n",
    "\n",
    "q_embed = embed_query(query_text)  \n",
    "results = collection.query(\n",
    "    query_embeddings=[q_embed],\n",
    "    n_results=5,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"],\n",
    ")\n",
    "\n",
    "docs      = results.get(\"documents\", [[]])[0]\n",
    "metadatas = results.get(\"metadatas\", [[]])[0]\n",
    "scores    = results.get(\"distances\", [[]])[0]\n",
    "\n",
    "for doc_text, meta, score in zip(docs, metadatas, scores):\n",
    "    filename = meta.get(\"filename\", \"unknown.pdf\")\n",
    "    page     = meta.get(\"page\", \"?\")\n",
    "    source   = meta.get(\"source\", \"\")\n",
    "    print(f\"Source: {filename} — page {page}  (distance={score:.4f})\")\n",
    "    if source:\n",
    "        print(f\"Link: {source}\")\n",
    "    print(doc_text[:400].strip(), \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44926384",
   "metadata": {},
   "source": [
    "### 4 — Refresh local mirror for BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de32a2f",
   "metadata": {},
   "source": [
    "rebulding a **local textual mirror** of the Chroma collection.  \n",
    "Because BM25 requires access to raw text (not embeddings), we:\n",
    "- Download all documents and metadata from Chroma in safe batches.\n",
    "- Tokenize each text to prepare for BM25 indexing.\n",
    "- Reconstruct a fresh BM25 model over the same chunk-level data stored in Chroma.\n",
    "\n",
    "This mirror should be refreshed whenever the collection content changes  \n",
    "(e.g., new PDFs ingested or existing ones updated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb7e326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 snapshot rebuilt | chunks=2922 | batch_size=300\n"
     ]
    }
   ],
   "source": [
    "def fetch_all_docs_from_chroma(\n",
    "    collection,\n",
    "    batch_size: int = CHROMA_GET_LIMIT,\n",
    "    include: Optional[List[str]] = None,\n",
    ") -> Dict[str, List[Any]]:\n",
    "    \"\"\"\n",
    "    Download all documents from a Chroma collection safely in batches.\n",
    "    Returns a dictionary with three parallel lists: 'ids', 'documents', and 'metadatas'.\n",
    "    This ensures a full local snapshot suitable for BM25 indexing.\n",
    "    \"\"\"\n",
    "    include = include or [\"documents\", \"metadatas\"]\n",
    "\n",
    "    # Initialize accumulators for all records\n",
    "    all_ids, all_docs, all_metas = [], [], []\n",
    "    offset = 0\n",
    "\n",
    "    while True:\n",
    "        # Fetch a batch of records from Chroma (using pagination)\n",
    "        res = collection.get(include=include, limit=batch_size, offset=offset)\n",
    "        ids = res.get(\"ids\", [])\n",
    "        if not ids:\n",
    "            # Exit loop when no more data is returned\n",
    "            break\n",
    "\n",
    "        # Retrieve corresponding documents and metadata\n",
    "        docs = res.get(\"documents\", [[]])\n",
    "        metas = res.get(\"metadatas\", [[]])\n",
    "\n",
    "        # Extend local buffers with the newly fetched data\n",
    "        all_ids.extend(ids)\n",
    "        all_docs.extend(docs)\n",
    "        all_metas.extend(metas)\n",
    "\n",
    "        # Move offset forward for the next batch\n",
    "        offset += len(ids)\n",
    "\n",
    "    # Return a unified snapshot of all data retrieved from Chroma\n",
    "    return {\"ids\": all_ids, \"documents\": all_docs, \"metadatas\": all_metas}\n",
    "\n",
    "\n",
    "# ---- Execute the full refresh process ----\n",
    "\n",
    "# Fetch all chunk texts and metadata from Chroma Cloud\n",
    "snapshot = fetch_all_docs_from_chroma(\n",
    "    collection,\n",
    "    batch_size=CHROMA_GET_LIMIT,\n",
    "    include=[\"documents\", \"metadatas\"],\n",
    ")\n",
    "\n",
    "# Unpack results into aligned lists\n",
    "all_ids   = snapshot[\"ids\"]\n",
    "all_texts = snapshot[\"documents\"]\n",
    "all_metas = snapshot[\"metadatas\"]\n",
    "\n",
    "# Tokenize the entire corpus (chunk-level texts)\n",
    "tokenized_corpus = [simple_tokenize(txt or \"\") for txt in all_texts]\n",
    "\n",
    "# Rebuild a new BM25 index from the tokenized corpus\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Confirmation log\n",
    "print(f\"BM25 snapshot rebuilt | chunks={len(all_ids)} | batch_size={CHROMA_GET_LIMIT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e028efa",
   "metadata": {},
   "source": [
    "**BM25** (short for Best Matching 25) is a classic keyword-based retrieval algorithm widely used in information retrieval systems. It provides a mathematical way to evaluate how relevant a document is to a given text query based purely on word overlap and term importance.\n",
    "\n",
    "**What does `bm25 = BM25Okapi(tokenized_corpus)` do?**\n",
    "\n",
    "This line builds a word-level index over a list of tokenized documents (in this case, all text chunks extracted from the PDFs). `tokenized_corpus` is a list of chunks, each represented as a list of words (tokens):\n",
    "\n",
    "<ul>\n",
    "    [\"rag\", \"retrieval\", \"augmentation\", \"generation\"]<br>\n",
    "    [\"graphrag\", \"entity\", \"graph\", \"relations\"]<br>\n",
    "    [\"raptor\", \"hierarchical\", \"clustering\", \"retrieval\"]<br>\n",
    "    ...\n",
    "</ul>\n",
    "\n",
    "Each chunk corresponds to a segment of text (often a paragraph or a few sentences) that was previously split and stored in Chroma.\n",
    "\n",
    "**How BM25 works internally**\n",
    "\n",
    "*Term Frequency (TF):* BM25 counts how many times each word appears in a chunk. Words that appear more often within a chunk contribute more to its score.\n",
    "\n",
    "*Inverse Document Frequency (IDF):* Words that appear in many chunks are considered less informative (e.g., \"model\", \"paper\"), while rare words like \"GraphRAG\" or \"RAPTOR\" get higher weight. This helps BM25 prioritize technical or distinctive terms.\n",
    "\n",
    "*Length Normalization:* Longer chunks naturally contain more words, so BM25 normalizes scores to prevent long texts from dominating the ranking unfairly.\n",
    "\n",
    "*Scoring:* When a query is given (e.g., \"GraphRAG entity extraction\"), BM25 compares the query words with the words in each chunk and assigns a relevance score. The more important and frequent the overlapping terms are, the higher the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68626d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_topk(query: str, k: int = 5):\n",
    "    \"\"\"Return the top-k most relevant chunks according to BM25.\"\"\"\n",
    "    # 1) Tokenize the query into simple words\n",
    "    tokens = simple_tokenize(query)\n",
    "    \n",
    "    # 2) Compute BM25 scores for all chunks\n",
    "    scores = bm25.get_scores(tokens)\n",
    "        \n",
    "    # 3) Sort results by descending score\n",
    "    top_idx = np.argsort(scores)[::-1][:k]\n",
    "    \n",
    "    # 4) Build structured results\n",
    "    results = []\n",
    "    for i in top_idx:\n",
    "        results.append({\n",
    "            \"id\": all_ids[i],\n",
    "            \"text\": all_texts[i],\n",
    "            \"metadata\": all_metas[i],\n",
    "            \"score\": float(scores[i]),\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db0489fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: 2501.15383v1.pdf — page 3\n",
      "Score: 49.31\n",
      "table 1 : model architecture and license of qwen2. 5 - 1m open - weight models. models layers heads ( q / kv ) tie embedding context / generation length license 7b 28 28 / 4 no 1m / 8k apache 2. 0 14b 48 40 / 8 no 1m / 8k apache 2. 0 3 pre - training long - context pre - training is computationally intensive and can be expensive. to enhance training efficiency and reduce costs, we focus on optimiz \n",
      "---\n",
      "\n",
      "Source: 2501.15383v1.pdf — page 2\n",
      "Score: 47.86\n",
      "qwen2. 5 - 14b - instruct - 1m. compared to the 128k versions, these models exhibit significantly enhanced long - context capabilities. additionally, we provide an api - accessible model based on mixture of experts ( moe ), called qwen2. 5 - turbo, which offers performance comparable to gpt - 4o - mini but with longer context, stronger capabilities, and more competitive pricing. beyond the models \n",
      "---\n",
      "\n",
      "Source: 2501.15383v1.pdf — page 15\n",
      "Score: 47.31\n",
      "its processing time from 4. 9 minutes to only 68 seconds. these improvements significantly reduce user waiting times for long - sequence tasks. compared to the open - source qwen2. 5 - 1m models, qwen2. 5 - turbo excels in short tasks and achieves competitive results on long - context tasks, while delivering shorter processing times and lower costs. consequently, it offers an excellent balance of \n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What did Qwen2.5-1M models do to enhance training efficiency and reduce costs ?\"\n",
    "results = bm25_topk(query, k=3)\n",
    "\n",
    "for r in results:\n",
    "    meta = r[\"metadata\"]\n",
    "    print(f\"Source: {meta.get('filename')} — page {meta.get('page')}\")\n",
    "    print(f\"Score: {r['score']:.2f}\")\n",
    "    print(r[\"text\"][:400].strip(), \"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a4a9c",
   "metadata": {},
   "source": [
    "#### How BM25 Retrieves Metadata\n",
    "\n",
    "BM25 itself only indexes and scores **text**, not metadata.\n",
    "\n",
    "When `bm25 = BM25Okapi(tokenized_corpus)` is created, it builds a word-based index over the text chunks. However, metadata like filename, page, and doc_id are stored in **parallel lists**:\n",
    "\n",
    "```python\n",
    "all_ids, all_texts, all_metas\n",
    "```\n",
    "\n",
    "Each index `i` refers to the same chunk across these lists. So when BM25 returns the best matches by index (`top_idx`), those indices are used to retrieve both the text and its metadata:\n",
    "\n",
    "```python\n",
    "for i in top_idx:\n",
    "    text = all_texts[i]\n",
    "    meta = all_metas[i]\n",
    "```\n",
    "\n",
    "That's why the BM25 results can include information like:\n",
    "\n",
    "```\n",
    "Source: 2502.11371v1.pdf — page 2\n",
    "Score: 22.24\n",
    "```\n",
    "\n",
    "Even though BM25 doesn't store metadata itself, the alignment by list position makes it possible to combine scores, text, and metadata seamlessly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e86e44",
   "metadata": {},
   "source": [
    "#### 5 — Dense search (Chroma)\n",
    "We query the Chroma collection using OpenAI embeddings for the **query**.\n",
    "Returns top-k hits with their ranks and useful metadata.\n",
    "5 — Dense search over Chroma (Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b672a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_search(query: str, k: int = 30) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve the top-k semantically relevant chunks from Chroma.\n",
    "    \n",
    "    Returns a list of dicts (one per hit) with a uniform schema:\n",
    "      - id:        chunk identifier (must match BM25 mirror to enable fusion)\n",
    "      - text:      chunk text\n",
    "      - metadata:  original metadata (filename, page, path, etc.)\n",
    "      - rank:      1-based rank within this retriever\n",
    "      - source:    'dense' (tag for downstream fusion)\n",
    "      - score:     optional monotonic score for diagnostics (higher is better)\n",
    "      - distance:  raw distance from Chroma (lower is better for L2)\n",
    "    \n",
    "    Notes:\n",
    "    - 'k' may be tuned based on corpus size (typical values: 20–50).\n",
    "    - Uses query_embeddings (client-side) to avoid dimension mismatches.\n",
    "    \"\"\"\n",
    "    q_emb = embed_query(query)  # 1536-dim with text-embedding-3-small\n",
    "    res = collection.query( # result already sorted by distance\n",
    "        query_embeddings=[q_emb],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "    )\n",
    "\n",
    "    hits: List[Dict[str, Any]] = []\n",
    "    ids   = res.get(\"ids\", [[]])[0] # the [0] -> first query only because chroma supports multiple queries\n",
    "    docs  = res.get(\"documents\", [[]])[0]\n",
    "    metas = res.get(\"metadatas\", [[]])[0]\n",
    "    dists = res.get(\"distances\", [[]])[0]\n",
    "\n",
    "    for i in range(len(ids)):\n",
    "        dist = float(dists[i])\n",
    "        hits.append({\n",
    "            \"id\": ids[i],\n",
    "            \"text\": docs[i],\n",
    "            \"metadata\": metas[i],\n",
    "            \"rank\": i + 1,           # 1-based\n",
    "            \"source\": \"dense\",\n",
    "            \"score\": -dist,          # invert distance so higher is better (diagnostic)\n",
    "            \"distance\": dist,        # raw distance (L2 or similar)\n",
    "        })\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff976701",
   "metadata": {},
   "source": [
    "#### 6 — BM25 Lexical Search (Local Mirror)\n",
    "\n",
    "The function below queries the BM25 index built over the local mirror of chunk texts\n",
    "(all_texts, all_metas, all_ids). It returns the same schema as dense_search,\n",
    "ensuring both retrievers can be fused consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f69342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_search(query: str, k: int = 30) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve the top-k lexically relevant chunks using the local BM25 index.\n",
    "    \n",
    "    Returns a list of dicts (one per hit) with the same schema as 'dense_search':\n",
    "      - id, text, metadata, rank (1-based), source='bm25', score (BM25), ...\n",
    "    \n",
    "    Assumes:\n",
    "      - 'bm25' is a BM25Okapi instance built over tokenized 'all_texts'\n",
    "      - 'all_ids', 'all_texts', 'all_metas' are parallel lists with identical order\n",
    "    \"\"\"\n",
    "    tokens = simple_tokenize(query)\n",
    "    scores = bm25.get_scores(tokens)  # one score per chunk (aligned with all_texts)\n",
    "    if len(scores) == 0:\n",
    "        return []\n",
    "\n",
    "    top_idx = np.argsort(scores)[::-1][:k]  # highest scores first\n",
    "    hits: List[Dict[str, Any]] = []\n",
    "    for r, i in enumerate(top_idx, start=1):\n",
    "        hits.append({\n",
    "            \"id\": all_ids[i],\n",
    "            \"text\": all_texts[i],\n",
    "            \"metadata\": all_metas[i],\n",
    "            \"rank\": r,               # 1-based\n",
    "            \"source\": \"bm25\",\n",
    "            \"score\": float(scores[i])  # higher is better for BM25\n",
    "        })\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fd4153",
   "metadata": {},
   "source": [
    "#### 7 — Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "RRF combines ranked lists from different retrievers (BM25 + Dense) into a single robust ranking,\n",
    "without needing to normalize scores. It uses ranks only, which makes it simple and effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e2376a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf_fuse(\n",
    "    bm25_hits: List[Dict[str, Any]],\n",
    "    dense_hits: List[Dict[str, Any]],\n",
    "    k: int = 10,\n",
    "    k_rrf: int = 60,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fuse BM25 and Dense ranked lists via Reciprocal Rank Fusion (RRF).\n",
    "    \n",
    "    RRF_score(d) = 1/(k_rrf + rank_bm25(d)) + 1/(k_rrf + rank_dense(d))\n",
    "    \n",
    "    Inputs:\n",
    "      - bm25_hits, dense_hits: lists with uniform schema (id, text, metadata, rank, source, ...)\n",
    "      - k: number of fused results to return\n",
    "      - k_rrf: typical constant (e.g., 60). Larger values reduce the dominance of top-1 items.\n",
    "    \n",
    "    Output:\n",
    "      - A list of fused results sorted by 'rrf_score' (desc), each containing:\n",
    "          id, text, metadata, rrf_score, and 'sources' with per-retriever ranks.\n",
    "    \"\"\"\n",
    "    # Build maps: id -> rank (1-based)\n",
    "    rank_bm25  = {h[\"id\"]: h[\"rank\"] for h in bm25_hits}\n",
    "    rank_dense = {h[\"id\"]: h[\"rank\"] for h in dense_hits}\n",
    "\n",
    "    # Universe of candidate IDs from both retrievers\n",
    "    all_ids_set = set(rank_bm25.keys()) | set(rank_dense.keys())\n",
    "\n",
    "    fused: List[Dict[str, Any]] = [] \n",
    "    for _id in all_ids_set:\n",
    "        r_b = rank_bm25.get(_id) # returns None if _id not in bm25\n",
    "        r_d = rank_dense.get(_id)\n",
    "\n",
    "        # Compute RRF score\n",
    "        rrf_score = 0.0\n",
    "        if r_b is not None:\n",
    "            rrf_score += 1.0 / (k_rrf + r_b)\n",
    "        if r_d is not None:\n",
    "            rrf_score += 1.0 / (k_rrf + r_d)\n",
    "\n",
    "        # Find the first result dictionary (h) whose \"id\" matches _id.\n",
    "        # The next() function scans the list and returns the first match, or None if not found.\n",
    "        # Prefer the version from dense_hits; if not present there, take it from bm25_hits.\n",
    "        rep = (\n",
    "            next((h for h in dense_hits if h[\"id\"] == _id), None) \n",
    "            or next((h for h in bm25_hits if h[\"id\"] == _id), None)\n",
    "        )\n",
    "\n",
    "        fused.append({\n",
    "            \"id\": _id,\n",
    "            \"text\": rep[\"text\"],\n",
    "            \"metadata\": rep[\"metadata\"],\n",
    "            \"rrf_score\": rrf_score,\n",
    "            \"sources\": {\n",
    "                \"bm25_rank\": r_b,\n",
    "                \"dense_rank\": r_d,\n",
    "            },\n",
    "        })\n",
    "\n",
    "    # Sort by fused score and truncate\n",
    "    fused.sort(key=lambda x: x[\"rrf_score\"], reverse=True)\n",
    "    return fused[:k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e23c9c",
   "metadata": {},
   "source": [
    "#### 8 — Hybrid Search Helper \n",
    "\n",
    "A convenience wrapper that runs both retrievers, fuses with RRF, and returns structured outputs.\n",
    "The example illustrates how to call it and print page-level citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "944fed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query: str, k_dense: int = 30, k_bm25: int = 30, k_final: int = 10) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run both retrieval modes (dense + BM25) and fuse results via RRF.\n",
    "    \n",
    "    Returns a dict with:\n",
    "      - 'bm25_hits': top-k BM25 results\n",
    "      - 'dense_hits': top-k Dense results\n",
    "      - 'fused': RRF-fused top-k_final results\n",
    "    \"\"\"\n",
    "    bm25_hits  = bm25_search(query, k=k_bm25)\n",
    "    dense_hits = dense_search(query, k=k_dense)\n",
    "    fused      = rrf_fuse(bm25_hits, dense_hits, k=k_final, k_rrf=60)\n",
    "    return {\"bm25_hits\": bm25_hits, \"dense_hits\": dense_hits, \"fused\": fused}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede9a168",
   "metadata": {},
   "source": [
    "##### 9 — Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16663df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================================================================================================\n",
      "  QUERY: What did Qwen2.5-1M models do to enhance training efficiency and reduce costs ?\n",
      "==============================================================================================================\n",
      "RANK  PDF FILE                                 PAGE   RRF SCORE  BM25   DENSE   SOURCE\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "1     2501.15383v1.pdf                         15     0.0323     3      1       c:\\Users\\Zakaria\\Downloads\\bm25-dense-etrieval\\data\\2501.15383v1.pdf#page=15\n",
      "        its processing time from 4. 9 minutes to only 68 seconds. these improvements significantly reduce user waiting times for long - sequence tasks. compared to the open - source qwen2. 5 - 1m models, qwen2. 5 - turbo excels in short tasks and achieves competitive results on long - context tasks, while d...\n",
      "\n",
      "2     2501.15383v1.pdf                         2      0.0323     2      2       c:\\Users\\Zakaria\\Downloads\\bm25-dense-etrieval\\data\\2501.15383v1.pdf#page=2\n",
      "        qwen2. 5 - 14b - instruct - 1m. compared to the 128k versions, these models exhibit significantly enhanced long - context capabilities. additionally, we provide an api - accessible model based on mixture of experts ( moe ), called qwen2. 5 - turbo, which offers performance comparable to gpt - 4o - m...\n",
      "\n",
      "3     2501.15383v1.pdf                         3      0.0320     1      4       c:\\Users\\Zakaria\\Downloads\\bm25-dense-etrieval\\data\\2501.15383v1.pdf#page=3\n",
      "        table 1 : model architecture and license of qwen2. 5 - 1m open - weight models. models layers heads ( q / kv ) tie embedding context / generation length license 7b 28 28 / 4 no 1m / 8k apache 2. 0 14b 48 40 / 8 no 1m / 8k apache 2. 0 3 pre - training long - context pre - training is computationally...\n",
      "\n",
      "4     2501.15383v1.pdf                         3      0.0303     6      6       c:\\Users\\Zakaria\\Downloads\\bm25-dense-etrieval\\data\\2501.15383v1.pdf#page=3\n",
      "        also reduces the overall computational cost by accelerating the learning process and requiring fewer iterations to achieve high performance. training strategy. training with long contexts requires substantial gpu memory, thus posing a severe challenge to both training costs and time. to improve trai...\n",
      "\n",
      "5     2501.15383v1.pdf                         2      0.0302     10     3       c:\\Users\\Zakaria\\Downloads\\bm25-dense-etrieval\\data\\2501.15383v1.pdf#page=2\n",
      "        qwen2. 5 - 1m series are developed based on qwen2. 5 models ( yang et al., 2025 ) and support context length up to 1m tokens. it currently includes two dense models for opensource, namely qwen2. 5 - 7b - 1m, qwen2. 5 - 14b - 1m, and a moe model for api service, namely qwen2. 5 - turbo. the qwen2. 5...\n",
      "\n",
      "==============================================================================================================\n",
      "Total results displayed: 5\n",
      "==============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "query = \"What did Qwen2.5-1M models do to enhance training efficiency and reduce costs ?\"\n",
    "result = hybrid_search(query, k_dense=30, k_bm25=30, k_final=5)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 110)\n",
    "print(f\"  QUERY: {query}\")\n",
    "print(\"=\" * 110)\n",
    "print(f\"{'RANK':<5} {'PDF FILE':<40} {'PAGE':<6} {'RRF SCORE':<10} {'BM25':<6} {'DENSE':<6}  SOURCE\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "for i, h in enumerate(result[\"fused\"], start=1):\n",
    "    m = h[\"metadata\"] or {}\n",
    "    filename = m.get(\"filename\", \"unknown.pdf\")\n",
    "    page = m.get(\"page\", \"?\")\n",
    "    rrf = h.get(\"rrf_score\", 0.0)\n",
    "    bm25_r = (h.get(\"sources\") or {}).get(\"bm25_rank\", None)\n",
    "    dense_r = (h.get(\"sources\") or {}).get(\"dense_rank\", None)\n",
    "\n",
    "    # Prefer 'source' if present (usually \"path#page=N\"), else build it from 'path' + page.\n",
    "    source = m.get(\"source\")\n",
    "    if not source:\n",
    "        path = m.get(\"path\", \"\")\n",
    "        source = f\"{path}#page={page}\" if path else \"(no source link)\"\n",
    "\n",
    "    # Clean preview (single line, truncated)\n",
    "    text_preview = (h[\"text\"][:300].strip().replace(\"\\n\", \" \") + \"...\")\n",
    "\n",
    "    # Row with columns\n",
    "    print(f\"{i:<5} {filename:<40} {str(page):<6} {rrf:<10.4f} {str(bm25_r or '-'): <6} {str(dense_r or '-'): <6}  {source}\")\n",
    "    # Indented preview\n",
    "    print(f\"        {text_preview}\\n\")\n",
    "\n",
    "print(\"=\" * 110)\n",
    "print(f\"Total results displayed: {len(result['fused'])}\")\n",
    "print(\"=\" * 110)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d99615d",
   "metadata": {},
   "source": [
    "# 10 - Analysis of Retrieval Performance Across Methods\n",
    "\n",
    "## Context\n",
    "\n",
    "All three retrieval pipelines were tested on the same query:\n",
    "\n",
    "**\"What did Qwen2.5-1M models do to enhance training efficiency and reduce costs?\"**\n",
    "\n",
    "The relevant information appears explicitly on page 3 of the paper `2501.15383v1.pdf`, in the sentence:\n",
    "\n",
    "> \"To enhance training efficiency and reduce costs, we focus on optimizing data efficiency and refining training strategies during the pre-training process of Qwen2.5-1M models.\"\n",
    "\n",
    "However, the three retrieval modes produced slightly different rankings for this passage.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Dense Retrieval (Embedding-based)\n",
    "\n",
    "### Mechanism\n",
    "\n",
    "- Uses OpenAI embeddings (`text-embedding-3-small`) to represent both query and chunks in a 1536-dimensional semantic space.\n",
    "- Measures semantic similarity between vectors (lower distance = higher similarity).\n",
    "\n",
    "### Result\n",
    "\n",
    "- Top results were semantically related but not always lexically exact.\n",
    "- Pages 15 and 2 dominated because they discussed training cost and efficiency but in broader contexts (e.g., latency, Turbo model performance).\n",
    "- The truly exact sentence on page 3 appeared around rank 4.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "Dense retrieval captures meaning, not exact wording. It tends to surface conceptually similar text, even when it doesn't include the same phrase.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. BM25 Lexical Retrieval\n",
    "\n",
    "### Mechanism\n",
    "\n",
    "- Uses term frequency and inverse document frequency (TF–IDF-like weighting).\n",
    "- Rewards exact word overlap between query and text.\n",
    "\n",
    "### Result\n",
    "\n",
    "- Page 3 ranked #1, because it contains the exact phrase \"enhance training efficiency and reduce costs\".\n",
    "- Pages 2 and 15 followed closely, since they reuse similar wording (training efficiency, cost reduction).\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "BM25 excels at finding exact lexical matches. It is precise for direct phrasing but can miss paraphrases or semantically related content.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Hybrid Retrieval (RRF Fusion of Dense + BM25)\n",
    "\n",
    "### Mechanism\n",
    "\n",
    "- Combines the semantic ranking (Dense) and lexical ranking (BM25) using Reciprocal Rank Fusion (RRF).\n",
    "- Each document's final score increases if it ranks highly in either method, and even more if in both.\n",
    "\n",
    "### Result\n",
    "\n",
    "- The fused top 5 were all relevant, showing stronger overlap between both systems.\n",
    "- However, page 3 sometimes dropped to rank #3, because Dense retrieval favored semantically similar paragraphs about \"efficiency\" and \"cost\" that used different wording.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "RRF fusion increases robustness but may still overvalue semantically broad hits unless weights are adjusted or lexical matches are boosted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f6c00",
   "metadata": {},
   "source": [
    "future things to do: \n",
    "1) Weighted RRF\n",
    "    w_bm25: float = 1.5,  \n",
    "    w_dense: float = 1.0\n",
    "2) Phrase/Proximity boost (mini re-ranker posterior a RRF)\n",
    "3) Ajuste de chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a929bf24",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
