{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a390b8a4",
   "metadata": {},
   "source": [
    "# Hybrid Retrieval (BM25 + Dense) with RRF — Concept Overview\n",
    "\n",
    "When a user asks a question, we need to **search through documents** (e.g., uv.es university content).\n",
    "\n",
    "There are two popular ways to search:\n",
    "\n",
    "- **BM25 (sparse retrieval)** — based on **exact word matching**.  \n",
    "  It works great when the query and the document share the same words  \n",
    "  *(e.g., “matrícula”, “plazos”, “UV”)*.\n",
    "\n",
    "- **Dense retrieval (embeddings)** — based on **semantic meaning**.  \n",
    "  Even if the words differ, it can match similar meanings  \n",
    "  *(e.g., “inscripción” ≈ “matrícula”)*.\n",
    "\n",
    "Each method fails where the other succeeds.  \n",
    "The usual solution is to **combine both**, increasing recall without sacrificing precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6ad584",
   "metadata": {},
   "source": [
    "# What is a Hybrid Retriever?\n",
    "\n",
    "A **hybrid retriever** runs **two searches in parallel**:\n",
    "\n",
    "1. One using **BM25** — ranking documents by **textual relevance**.\n",
    "2. Another using **embeddings** — ranking by **semantic similarity**.\n",
    "\n",
    "Finally, it **merges both ranked lists** into a single, more reliable ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772bc0f1",
   "metadata": {},
   "source": [
    "# RRF (Reciprocal Rank Fusion)\n",
    "\n",
    "**Reciprocal Rank Fusion (RRF)** is a simple yet powerful rule for combining ranked lists.\n",
    "\n",
    "### Inputs:\n",
    "- **List A** (e.g., BM25) returns documents with ranks: `rank_A(d) = 1, 2, 3, ...` (1 = best)\n",
    "- **List B** (e.g., Dense) returns ranks: `rank_B(d) = 1, 2, 3, ...`\n",
    "\n",
    "### Formula (per document `d`):\n",
    "RRF\\_score(d) = 1/(k + rank_A(d)) + 1/(k + rank_B(d))\n",
    "\n",
    "\n",
    "- **k** is a constant (typically `60`) — it ensures top ranks matter more but prevents one list from dominating.\n",
    "- If a document doesn’t appear in a list, we treat it as having a **very large rank** (or simply no contribution).\n",
    "\n",
    "### Intuition:\n",
    "- A document ranked very high in **either** list gets a **strong boost** (`1/(k+1)`).\n",
    "- A document that ranks moderately in **both** lists also gains a solid combined score.\n",
    "- **RRF is robust** — no need to normalize scores or tune probabilities, just use ranks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d18acc",
   "metadata": {},
   "source": [
    "# Simple Example of RRF\n",
    "\n",
    "Assume we take the **top-3** documents from each retriever:\n",
    "\n",
    "**BM25 (List A)** → `d1`, `d2`, `d3`  \n",
    "**Dense (List B)** → `d2`, `d3`, `d4`  \n",
    "\n",
    "We set `k = 60` (a common choice).\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-step computation\n",
    "\n",
    "| Document | rank_A | rank_B | RRF Score Calculation | Total |\n",
    "|-----------|--------|--------|------------------------|--------|\n",
    "| **d1** | 1 | — | 1/(60+1) + 0 | **1/61** |\n",
    "| **d2** | 2 | 1 | 1/(60+2) + 1/(60+1) | **1/62 + 1/61** |\n",
    "| **d3** | 3 | 2 | 1/(60+3) + 1/(60+2) | **1/63 + 1/62** |\n",
    "| **d4** | — | 3 | 0 + 1/(60+3) | **1/63** |\n",
    "\n",
    "---\n",
    "\n",
    "### Final order (highest to lowest RRF score):\n",
    "1. **d2** → highest combined (top in both lists)  \n",
    "2. **d3** → solid mid-rank in both lists  \n",
    "3. **d1** → strong in BM25 only  \n",
    "4. **d4** → found only in Dense search\n",
    "\n",
    "**d2 wins** because it performs well in *both* search types.  \n",
    "**d1** was best in BM25 but missing in Dense, so it drops behind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2bb106",
   "metadata": {},
   "source": [
    "### 1 — Environment & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84e25f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment & Imports\n",
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "# BM25\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import re\n",
    "\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "\n",
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    SentenceTransformersTokenTextSplitter,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# configuration expected from .env.\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "CHROMADB_TOKEN = os.environ.get(\"CHROMADB_TOKEN\")\n",
    "CHROMA_TENANT = os.environ.get(\"CHROMA_TENANT\")       \n",
    "CHROMA_DB = os.environ.get(\"CHROMA_DB\")             \n",
    "CHROMA_HOST = os.environ.get(\"CHROMA_HOST\")            \n",
    "CHROMA_COLLECTION = os.environ.get(\"CHROMA_COLLECTION\") \n",
    "\n",
    "# Early fail-fast to surface configuration errors deterministically.\n",
    "assert OPENAI_API_KEY, \"Set OPENAI_API_KEY\"\n",
    "assert CHROMADB_TOKEN and CHROMA_TENANT and CHROMA_DB and CHROMA_HOST and CHROMA_COLLECTION, \"Set Chroma Cloud env vars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa8f1a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI client\n",
    "llm = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Chroma client (Cloud)\n",
    "client = chromadb.CloudClient(\n",
    "    api_key=CHROMADB_TOKEN,\n",
    "    tenant=CHROMA_TENANT,\n",
    "    database=CHROMA_DB,\n",
    ")\n",
    "collection = client.get_or_create_collection(name=CHROMA_COLLECTION)\n",
    "\n",
    "# Minimal tokenizer dedicated to BM25: lowercasing + simple word/symbol segmentation.\n",
    "# this is sufficient for BM25’s bag-of-words scoring.\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    return re.findall(r\"\\w+|\\S\", (text or \"\").lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5638a813",
   "metadata": {},
   "source": [
    "### 2 - Embedding helpers (OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c8f17b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-3-small\"  \n",
    "\n",
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Vectorize a batch of texts using OpenAI embeddings. \n",
    "    Returns one embedding per input text, preserving order.\"\"\"\n",
    "    if not texts:\n",
    "        return []\n",
    "    # Send the texts to the OpenAI embedding model to generate embeddings.\n",
    "    # 'resp' is an API response object containing metadata + one embedding per text.\n",
    "    resp = llm.embeddings.create(model=EMBEDDING_MODEL, input=texts)\n",
    "    # Extract only the embedding vectors from the response (ignore metadata).\n",
    "    # resp.data is a list of objects; each has an attribute 'embedding' which is a list of floats.\n",
    "    return [d.embedding for d in resp.data]\n",
    "\n",
    "def embed_query(query: str) -> List[float]:\n",
    "    \"\"\"Vectorize a single query (thin wrapper over embed_texts).\"\"\"\n",
    "    return embed_texts([query])[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c148b54",
   "metadata": {},
   "source": [
    "### 3 — PDF Ingestion, Two-Pass Chunking, Embedding, and Upsert to Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1108c3",
   "metadata": {},
   "source": [
    "#### Scan `./data` and extract text from each PDF\n",
    "\n",
    "For each PDF:\n",
    "- read pages, extract text\n",
    "- join per-document text with double newlines\n",
    "- build a minimal metadata record\n",
    "- keep both full document text and per-page text for traceability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c4bddd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25 PDFs under ./data\n",
      "Ingested 25 PDF documents with text.\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "assert os.path.isdir(DATA_DIR), f\"Data folder not found: {DATA_DIR}\"\n",
    "\n",
    "pdf_paths = sorted(glob.glob(os.path.join(DATA_DIR, \"**/*.pdf\"), recursive=True))\n",
    "print(f\"Found {len(pdf_paths)} PDFs under {DATA_DIR}\")\n",
    "\n",
    "raw_docs: List[Dict[str, Any]] = []  # items: {\"doc_id\",\"filename\",\"path\",\"text\",\"pages_text\",\"last_modified\"}\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipping {pdf_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Extract per-page text (strip leading/trailing whitespace)\n",
    "    pages_text: List[str] = []\n",
    "    for p in reader.pages:\n",
    "        t = p.extract_text() or \"\"\n",
    "        t = t.strip()\n",
    "        if t:\n",
    "            pages_text.append(t)\n",
    "\n",
    "    # Join all page texts into a single document text\n",
    "    full_text = \"\\n\\n\".join(pages_text)\n",
    "\n",
    "    # Skip empty documents (e.g., scanned PDFs without OCR)\n",
    "    if not full_text.strip():\n",
    "        print(f\"[WARN] Empty text: {pdf_path}\")\n",
    "        continue\n",
    "\n",
    "    # Metadata\n",
    "    stat = os.stat(pdf_path)\n",
    "    last_modified = datetime.fromtimestamp(stat.st_mtime).isoformat()\n",
    "    doc_id = str(uuid4())  # alternatively, derive a stable id from filepath+mtime if desired\n",
    "\n",
    "    raw_docs.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"filename\": os.path.basename(pdf_path),\n",
    "        \"path\": os.path.abspath(pdf_path),\n",
    "        \"text\": full_text,\n",
    "        \"pages_text\": pages_text,  # retain per-page traceability\n",
    "        \"last_modified\": last_modified,\n",
    "    })\n",
    "\n",
    "print(f\"Ingested {len(raw_docs)} PDF documents with text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7165aa6",
   "metadata": {},
   "source": [
    "#### Two-pass splitting\n",
    "First pass at character level preserves coarse structure and avoids awkward breaks.\n",
    "\n",
    "Second pass at token level introduces overlap to improve recall near boundaries.\n",
    "\n",
    "character chunks ≈ 1000 (no overlap)\n",
    "\n",
    "token chunks ≈ 384 tokens with 64-token overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe560917",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "token_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    tokens_per_chunk=384,\n",
    "    chunk_overlap=64,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b053c92",
   "metadata": {},
   "source": [
    "#### Split documents into page-aware chunks with rich metadata\n",
    "\n",
    "For each document:\n",
    "\n",
    "iterate page by page\n",
    "\n",
    "apply character-level splitting, then token-level splitting\n",
    "\n",
    "construct chunks with metadata capturing the source PDF and page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a2f18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks_by_page(doc: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Produce page-aware chunks from a document.\n",
    "    Pipeline: page -> character-level segments -> token-level chunks (with overlap).\n",
    "    Each chunk carries metadata sufficient for page-level citation and source tracing.\n",
    "    \"\"\"\n",
    "    chunks: List[Dict[str, Any]] = []\n",
    "    pages: List[str] = doc.get(\"pages_text\") or [doc[\"text\"]]\n",
    "    total_pages = len(pages)\n",
    "\n",
    "    global_chunk_idx = 0  # running index of chunks across the whole document\n",
    "\n",
    "    for page_num, page_text in enumerate(pages, start=1):\n",
    "        if not page_text or not page_text.strip():\n",
    "            continue\n",
    "\n",
    "        # First pass: character-level segmentation\n",
    "        char_segments = char_splitter.split_text(page_text)\n",
    "\n",
    "        # Second pass: token-level segmentation with overlap\n",
    "        page_chunk_idx = 0\n",
    "        for seg in char_segments:\n",
    "            token_chunks = token_splitter.split_text(seg)\n",
    "            for chunk_text in token_chunks:\n",
    "                # Skip degenerate chunks that are purely whitespace\n",
    "                if not chunk_text.strip():\n",
    "                    continue\n",
    "\n",
    "                chunk_id = f\"{doc['doc_id']}::p{page_num}::ch{global_chunk_idx}\"\n",
    "\n",
    "                chunks.append({\n",
    "                    \"id\": chunk_id,\n",
    "                    \"text\": chunk_text,\n",
    "                    \"metadata\": {\n",
    "                        \"doc_id\": doc[\"doc_id\"],\n",
    "                        \"filename\": doc[\"filename\"],\n",
    "                        \"path\": doc[\"path\"],\n",
    "                        \"page\": page_num,                    # 1-based page number for citation\n",
    "                        \"page_chunk_index\": page_chunk_idx,  # chunk index within the page\n",
    "                        \"chunk_index\": global_chunk_idx,     # global chunk index within the doc\n",
    "                        \"total_pages\": total_pages,\n",
    "                        \"last_modified\": doc[\"last_modified\"],\n",
    "                        # Useful anchor for viewers that support page fragments:\n",
    "                        \"source\": f\"{doc['path']}#page={page_num}\",\n",
    "                        # Additional domain-specific fields can be added here (e.g., \"section\", \"language\").\n",
    "                    }\n",
    "                })\n",
    "\n",
    "                page_chunk_idx += 1\n",
    "                global_chunk_idx += 1\n",
    "\n",
    "    print(f\"[{doc['filename']}] produced {len(chunks)} chunks with page metadata.\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9e952b",
   "metadata": {},
   "source": [
    "#### Build the complete chunk list across the corpus\n",
    "\n",
    "Aggregate page-aware chunks from all documents into a single list for embedding and upsert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e6ee450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024.findings-acl.456.pdf] produced 89 chunks with page metadata.\n",
      "[2025.acl-long.366.pdf] produced 111 chunks with page metadata.\n",
      "[2401.10020v3.pdf] produced 80 chunks with page metadata.\n",
      "[2401.10774v3.pdf] produced 107 chunks with page metadata.\n",
      "[2401.15884v3.pdf] produced 69 chunks with page metadata.\n",
      "[2401.18059v1.pdf] produced 92 chunks with page metadata.\n",
      "[2402.01306v4.pdf] produced 95 chunks with page metadata.\n",
      "[2402.13547v2.pdf] produced 88 chunks with page metadata.\n",
      "[2402.13753v1.pdf] produced 78 chunks with page metadata.\n",
      "[2403.03206v1.pdf] produced 111 chunks with page metadata.\n",
      "[2403.19887v2.pdf] produced 57 chunks with page metadata.\n",
      "[2404.16130v2.pdf] produced 105 chunks with page metadata.\n",
      "[2405.04437v3.pdf] produced 109 chunks with page metadata.\n",
      "[2405.14734v3.pdf] produced 131 chunks with page metadata.\n",
      "[2405.21060v1.pdf] produced 216 chunks with page metadata.\n",
      "[2406.04692v1.pdf] produced 61 chunks with page metadata.\n",
      "[2407.08608v2.pdf] produced 81 chunks with page metadata.\n",
      "[2407.21783v3.pdf] produced 423 chunks with page metadata.\n",
      "[2408.00714v2.pdf] produced 198 chunks with page metadata.\n",
      "[2408.03326v3.pdf] produced 184 chunks with page metadata.\n",
      "[2411.02265v3.pdf] produced 77 chunks with page metadata.\n",
      "[2412.03603v6.pdf] produced 117 chunks with page metadata.\n",
      "[2501.15383v1.pdf] produced 85 chunks with page metadata.\n",
      "[2502.11371v1.pdf] produced 69 chunks with page metadata.\n",
      "[2503.20215v1.pdf] produced 89 chunks with page metadata.\n",
      "Total chunks prepared: 2922\n"
     ]
    }
   ],
   "source": [
    "all_chunks: List[Dict[str, Any]] = []\n",
    "for d in raw_docs:\n",
    "    all_chunks.extend(split_into_chunks_by_page(d))\n",
    "\n",
    "print(f\"Total chunks prepared: {len(all_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b79a5e",
   "metadata": {},
   "source": [
    "#### Embed and upsert chunks into Chroma (batched)\n",
    "\n",
    "Procedure:\n",
    "\n",
    "batch the chunk list to respect rate limits\n",
    "\n",
    "compute embeddings for each batch\n",
    "\n",
    "upsert (ids, documents, embeddings, metadatas) into the target Chroma collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0294b0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 2922 chunks into Chroma\n"
     ]
    }
   ],
   "source": [
    "BATCH = 256  # tune based on provider limits and latency characteristics\n",
    "\n",
    "def batched(iterable, n: int):\n",
    "    \"\"\"Yield successive n-sized lists from iterable.\"\"\"\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i : i + n]\n",
    "\n",
    "upserted = 0\n",
    "for batch in batched(all_chunks, BATCH):\n",
    "    texts = [c[\"text\"] for c in batch]\n",
    "    ids   = [c[\"id\"] for c in batch]\n",
    "    metas = [c[\"metadata\"] for c in batch]\n",
    "\n",
    "    # Embeddings (OpenAI-compatible)\n",
    "    embeds = embed_texts(texts)\n",
    "\n",
    "    # Upsert into Chroma\n",
    "    collection.upsert(\n",
    "        ids=ids,\n",
    "        documents=texts,\n",
    "        embeddings=embeds,\n",
    "        metadatas=metas,\n",
    "    )\n",
    "    upserted += len(batch)\n",
    "\n",
    "print(f\"Upserted {upserted} chunks into Chroma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1000f337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: 2501.15383v1.pdf — page 15  (distance=0.5904)\n",
      "Link: c:\\Users\\Zakaria\\Downloads\\bm25-dense-etrieval\\data\\2501.15383v1.pdf#page=15\n",
      "its processing time from 4. 9 minutes to only 68 seconds. these improvements significantly reduce user waiting times for long - sequence tasks. compared to the open - source qwen2. 5 - 1m models, qwen2. 5 - turbo excels in short tasks and achieves competitive results on long - context tasks, while delivering shorter processing times and lower costs. consequently, it offers an excellent balance of ...\n",
      "\n",
      "Source: 2501.15383v1.pdf — page 2  (distance=0.6577)\n",
      "Link: c:\\Users\\Zakaria\\Downloads\\bm25-dense-etrieval\\data\\2501.15383v1.pdf#page=2\n",
      "qwen2. 5 - 14b - instruct - 1m. compared to the 128k versions, these models exhibit significantly enhanced long - context capabilities. additionally, we provide an api - accessible model based on mixture of experts ( moe ), called qwen2. 5 - turbo, which offers performance comparable to gpt - 4o - mini but with longer context, stronger capabilities, and more competitive pricing. beyond the models ...\n",
      "\n",
      "Source: 2501.15383v1.pdf — page 2  (distance=0.7265)\n",
      "Link: c:\\Users\\Zakaria\\Downloads\\bm25-dense-etrieval\\data\\2501.15383v1.pdf#page=2\n",
      "qwen2. 5 - 1m series are developed based on qwen2. 5 models ( yang et al., 2025 ) and support context length up to 1m tokens. it currently includes two dense models for opensource, namely qwen2. 5 - 7b - 1m, qwen2. 5 - 14b - 1m, and a moe model for api service, namely qwen2. 5 - turbo. the qwen2. 5 - 1m models retain the same transformer - based architecture as qwen2. 5, ensuring compati - bility ...\n",
      "\n",
      "Source: 2501.15383v1.pdf — page 3  (distance=0.7355)\n",
      "Link: c:\\Users\\Zakaria\\Downloads\\bm25-dense-etrieval\\data\\2501.15383v1.pdf#page=3\n",
      "table 1 : model architecture and license of qwen2. 5 - 1m open - weight models. models layers heads ( q / kv ) tie embedding context / generation length license 7b 28 28 / 4 no 1m / 8k apache 2. 0 14b 48 40 / 8 no 1m / 8k apache 2. 0 3 pre - training long - context pre - training is computationally intensive and can be expensive. to enhance training efficiency and reduce costs, we focus on optimiz ...\n",
      "\n",
      "Source: 2501.15383v1.pdf — page 4  (distance=0.7864)\n",
      "Link: c:\\Users\\Zakaria\\Downloads\\bm25-dense-etrieval\\data\\2501.15383v1.pdf#page=4\n",
      "sequences to fully realize their potential on relatively shorter tasks. 4 post - training the aim of post - training is to effectively enhance the model ’ s performance on long - context tasks while ensuring that performance on short tasks does not decline. we highlight the following efforts in building the qwen2. 5 - 1m models during post - training : synthesizing long instruction data. in long - ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_text = \"What did Qwen2.5-1M models do to enhance training efficiency and reduce costs?\"\n",
    "\n",
    "q_embed = embed_query(query_text)  \n",
    "results = collection.query(\n",
    "    query_embeddings=[q_embed],\n",
    "    n_results=5,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"],\n",
    ")\n",
    "\n",
    "docs      = results.get(\"documents\", [[]])[0]\n",
    "metadatas = results.get(\"metadatas\", [[]])[0]\n",
    "scores    = results.get(\"distances\", [[]])[0]\n",
    "\n",
    "for doc_text, meta, score in zip(docs, metadatas, scores):\n",
    "    filename = meta.get(\"filename\", \"unknown.pdf\")\n",
    "    page     = meta.get(\"page\", \"?\")\n",
    "    source   = meta.get(\"source\", \"\")\n",
    "    print(f\"Source: {filename} — page {page}  (distance={score:.4f})\")\n",
    "    if source:\n",
    "        print(f\"Link: {source}\")\n",
    "    print(doc_text[:400].strip(), \"...\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
